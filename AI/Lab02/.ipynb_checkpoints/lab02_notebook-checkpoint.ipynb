{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Sentiment Analysis\n",
    "## 2.2 Movie Review Data\n",
    "\n",
    "Let us first start by looking at the data provided with the exercise. We have positive and negative movie reviews labeled by human readers, all positive and negative reviews are in the ‘pos’ and ‘neg’ folders respectively. If you look in- side a sample file, you will see that these review messages have been ‘tokenized’, where all words are separated from punctuations.\n",
    "There are approximately 1000 files in each category with files names starting with cv000, cv001, cv002 and so on. You will split the dataset into training set and testing set.\n",
    "\n",
    "1. Write some code to load the data from text files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from text files\n",
    "    1. ใช้ glob เพื่ออ่านชื่อไฟล์ทั้งหมดใน directory\n",
    "    2. ใช้ with open ชื่อไฟล์นั้นๆ เพื่ออ่าน text files ที่อยู่ข้างใน แล้ว append ไปเก็บไว้ใน raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_txt = glob.glob( r'C:\\Users\\dell\\Desktop\\module8&9\\AI\\Lab02\\neg\\*.txt') #read file name in folder' neg\n",
    "pos_txt = glob.glob( r'C:\\Users\\dell\\Desktop\\module8&9\\AI\\Lab02\\pos\\*.txt') #read file name in folder' pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read data in txt.file and append it to list\n",
    "raw_data = []\n",
    "for i in range(len(pos_txt)):\n",
    "    with open(pos_txt[i],'r') as F: \n",
    "        raw_data.append(F.read())\n",
    "    \n",
    "for i in range(len(neg_txt)):\n",
    "    with open(neg_txt[i],'r') as F: \n",
    "        raw_data.append(F.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 TF-IDF\n",
    "From a raw text review, you want to create a vector, whose elements indicate the number of each word in each document. The frequency of all words within the documents are the ‘features’ of this machine learning problem.\n",
    "\n",
    "A popular method for transforming a text to a vector is called tf-idf, short for term frequencyinverse document frequency.\n",
    "\n",
    "1. Conduct a research about tf-idf and explain how it works.\n",
    "2. Scikit-learn provides a module for calculating this, this is called TfidfVec- torizer.\n",
    "You can study how this function is used here:\n",
    "\n",
    "`http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html`\n",
    "\n",
    "Write code to transform your text to tf-idf vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-idf\n",
    "    จะเป็นการหา frequency ของ word ต่างๆที่อยู่ใน raw_data ทั้งหมด แล้ว return ออกมาเป็น column ซึ่งขนาดของ column จะเท่ากับ จำนวน word ที่ไม่ซ้ำกันทั้งหมด\n",
    "โดยในแต่ละ column ค่าออกมาอยู่ในช่วง 0-1 \n",
    "        1. โดยถ้าเข้าใกล้ 1 หมายถึง มี word นั้นอยู่ใน text.file นั้นเยอะมาก \n",
    "        2. ถ้าเข้าใกล้ 0 หมายถึง มี word นั้นอยู่ใน text.file น้อยมาก"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "Tfid1 = TfidfVectorizer()\n",
    "feature1 = Tfid1.fit_transform(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(feature1.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>39649</th>\n",
       "      <th>39650</th>\n",
       "      <th>39651</th>\n",
       "      <th>39652</th>\n",
       "      <th>39653</th>\n",
       "      <th>39654</th>\n",
       "      <th>39655</th>\n",
       "      <th>39656</th>\n",
       "      <th>39657</th>\n",
       "      <th>39658</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.052257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39659 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0         1      2      3      4      5      6      7      8      9      \\\n",
       "0  0.052257  0.000000    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "1  0.000000  0.000000    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "2  0.000000  0.000000    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3  0.000000  0.020881    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4  0.000000  0.000000    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   ...    39649  39650  39651  39652  39653  39654  39655  39656  39657  39658  \n",
       "0  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "1  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "2  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "4  ...      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 39659 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# assign true class \n",
    "y = np.hstack((np.ones(1000),np.zeros(1000))) #1 = pos , 0 = neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Classification\n",
    "\n",
    "Use 4 different models to classify each movie into positive or negative category.\n",
    "\n",
    "1. K-Nearestneighbormodel,using module `sklearn.neighbors.KNeighborsClassifier`\n",
    "2. RandomForest, using module `sklearn.ensemble.RandomForestClassifier`\n",
    "3. SVM, using module `sklearn.svm.SVC`\n",
    "4. Neural network, using `sklearn.neural_network.MLPClassifier`\n",
    "\n",
    "You may pick other models you would like to try. Just present results for at least 4 models.\n",
    "Please provide your code for model fitting and cross validation. Calculate your classification accuracy, precision, and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------KN1 with feature1-----------------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.80      0.19      0.31      1000\n",
      "        1.0       0.54      0.95      0.69      1000\n",
      "\n",
      "avg / total       0.67      0.57      0.50      2000\n",
      "\n",
      "accuracy = 0.572\n"
     ]
    }
   ],
   "source": [
    "KN1 = KNeighborsClassifier()\n",
    "yhat_KN1 = cross_val_predict(KN1,feature1,y)\n",
    "scoreKN1 = accuracy_score(y,yhat_KN1)\n",
    "report = classification_report(y, yhat_KN1)\n",
    "print('--------------------------------KN1 with feature1-----------------------------------\\n')\n",
    "print(report)\n",
    "print('accuracy = ' + str(scoreKN1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------RF1 with feature1-----------------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.62      0.77      0.69      1000\n",
      "        1.0       0.70      0.53      0.60      1000\n",
      "\n",
      "avg / total       0.66      0.65      0.65      2000\n",
      "\n",
      "accuracy = 0.6515\n"
     ]
    }
   ],
   "source": [
    "RF1 = RandomForestClassifier()\n",
    "yhat_RF1 = cross_val_predict(RF1,feature1,y)\n",
    "scoreRF1 = accuracy_score(y,yhat_RF1)\n",
    "report = classification_report(y, yhat_RF1)\n",
    "print('--------------------------------RF1 with feature1-----------------------------------\\n')\n",
    "print(report)\n",
    "print('accuracy = ' + str(scoreRF1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------SVM1 with feature1-----------------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.80      0.70      0.74      1000\n",
      "        1.0       0.73      0.82      0.77      1000\n",
      "\n",
      "avg / total       0.76      0.76      0.76      2000\n",
      "\n",
      "accuracy = 0.7585\n"
     ]
    }
   ],
   "source": [
    "SVM1 = SVC()\n",
    "yhat_SVM1 = cross_val_predict(SVM1,feature1,y)\n",
    "scoreSVM1 = accuracy_score(y,yhat_SVM1)\n",
    "report = classification_report(y, yhat_SVM1)\n",
    "print('--------------------------------SVM1 with feature1-----------------------------------\\n')\n",
    "print(report)\n",
    "print('accuracy = ' + str(scoreSVM1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------MLP1 with feature1-----------------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.84      0.82      0.83      1000\n",
      "        1.0       0.82      0.84      0.83      1000\n",
      "\n",
      "avg / total       0.83      0.83      0.83      2000\n",
      "\n",
      "accuracy = 0.8275\n"
     ]
    }
   ],
   "source": [
    "MLP1 = MLPClassifier()\n",
    "yhat_MLP1 = cross_val_predict(MLP1,feature1,y)\n",
    "scoreMLP1 = accuracy_score(y,yhat_MLP1)\n",
    "report = classification_report(y, yhat_MLP1)\n",
    "print('--------------------------------MLP1 with feature1-----------------------------------\\n')\n",
    "print(report)\n",
    "print('accuracy = ' + str(scoreMLP1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Model Tuning\n",
    "\n",
    "Can you try to beat the simple model you created above? Here are some things you may try:\n",
    "\n",
    "* When creating TfidfVectorizer object, you may tweak sublinear_tf parameter which use the tf with logarithmic scale instead of the usual tf.\n",
    "* You may also exclude words that are too frequent or too rare, by adjusting max_df and min_df.\n",
    "* Adjusting parameters available in the model, like neural network structure or number of trees in the forest.\n",
    "\n",
    "Design at least 3 experiments using these techniques. Show your experimental results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNeighbors\n",
    "กำหนด max_df = 0.85 min_df = 0.1 \n",
    "    1. accuracy: 0.676 > 0.572\n",
    "    2. precission: 0.68 > 0.67\n",
    "    3. recall: 0.68 > 0.57\n",
    "เปลี่ยน hyper parameter ค่า k = 300 จาก Default = 5 โดย max_df = 0.85 min_df = 0.1 \n",
    "    1. accuracy: 0.727\n",
    "    2. precission: 0.74 \n",
    "    3. recall: 0.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------KN1 with feature2-----------------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.71      0.60      0.65      1000\n",
      "        1.0       0.65      0.75      0.70      1000\n",
      "\n",
      "avg / total       0.68      0.68      0.67      2000\n",
      "\n",
      "accuracy = 0.676\n"
     ]
    }
   ],
   "source": [
    "Tfid2 = TfidfVectorizer(max_df = 0.85,min_df = 0.1)\n",
    "feature2 = Tfid2.fit_transform(raw_data)\n",
    "yhat_KN2 = cross_val_predict(KN1,feature2,y)\n",
    "scoreKN2 = accuracy_score(y,yhat_KN2)\n",
    "report = classification_report(y, yhat_KN2)\n",
    "print('--------------------------------KN1 with feature2-----------------------------------\\n')\n",
    "print(report)\n",
    "print('accuracy = ' + str(scoreKN2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------KN1 with feature2-----------------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.69      0.84      0.75      1000\n",
      "        1.0       0.79      0.61      0.69      1000\n",
      "\n",
      "avg / total       0.74      0.73      0.72      2000\n",
      "\n",
      "accuracy = 0.727\n"
     ]
    }
   ],
   "source": [
    "KN2 = KNeighborsClassifier(n_neighbors = 300)\n",
    "yhat_KN2 = cross_val_predict(KN2,feature2,y)\n",
    "scoreKN2 = accuracy_score(y,yhat_KN2)\n",
    "report = classification_report(y, yhat_KN2)\n",
    "print('--------------------------------KN2 with feature2-----------------------------------\\n')\n",
    "print(report)\n",
    "print('accuracy = ' + str(scoreKN2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest\n",
    "กำหนด max_df = 0.85 min_df = 0.1 \n",
    "    1. accuracy: 0.6775 > 0.6515\n",
    "    2. precission: 0.68 > 0.66\n",
    "    3. recall: 0.68 > 0.65\n",
    "เปลี่ยน hyper parameter ค่า n_estimators = 300 จาก Default = 10 โดย max_df = 0.85 min_df = 0.1 \n",
    "    1. accuracy: 0.7855\n",
    "    2. precission: 0.79\n",
    "    3. recall: 0.79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------RF1 with feature2-----------------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.65      0.77      0.71      1000\n",
      "        1.0       0.72      0.58      0.64      1000\n",
      "\n",
      "avg / total       0.68      0.68      0.67      2000\n",
      "\n",
      "accuracy = 0.6775\n"
     ]
    }
   ],
   "source": [
    "Tfid2 = TfidfVectorizer(max_df = 0.85,min_df = 0.1)\n",
    "feature2 = Tfid2.fit_transform(raw_data)\n",
    "yhat_RF2 = cross_val_predict(RF1,feature2,y)\n",
    "scoreRF2 = accuracy_score(y,yhat_RF2)\n",
    "report = classification_report(y, yhat_RF2)\n",
    "print('--------------------------------RF1 with feature2-----------------------------------\\n')\n",
    "print(report)\n",
    "print('accuracy = ' + str(scoreRF2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------RF1 with feature2-----------------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.78      0.80      0.79      1000\n",
      "        1.0       0.79      0.77      0.78      1000\n",
      "\n",
      "avg / total       0.79      0.79      0.79      2000\n",
      "\n",
      "accuracy = 0.7855\n"
     ]
    }
   ],
   "source": [
    "RF2 = RandomForestClassifier(n_estimators = 1000)\n",
    "yhat_RF2 = cross_val_predict(RF2,feature2,y)\n",
    "scoreRF2 = accuracy_score(y,yhat_RF2)\n",
    "report = classification_report(y, yhat_RF2)\n",
    "print('--------------------------------RF2 with feature2-----------------------------------\\n')\n",
    "print(report)\n",
    "print('accuracy = ' + str(scoreRF2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "กำหนด max_df = 0.85 min_df = 0.1 \n",
    "    1. accuracy: 0.7285 < 0.7585\n",
    "    2. precission: 0.73 < 0.76\n",
    "    3. recall: 0.73 < 0.76\n",
    "    **max_df และ min_df ทำให้ performance ลดลง \n",
    "เปลี่ยน hyper parameter ค่า kernel = 'sigmoid' จาก Default = 'rbf' โดย max_df,min_df (default)\n",
    "    1. accuracy: 0.7585\n",
    "    2. precission: 0.76\n",
    "    3. recall: 0.76\n",
    "    **ผลลัพธ์ไม่ต่าง\n",
    "เปลี่ยน hyper parameter ค่า kernel = 'linear' จาก Default = 'rbf' โดย max_df,min_df (default)\n",
    "    1. accuracy: 0.845\n",
    "    2. precission: 0.85\n",
    "    3. recall: 0.84\n",
    "    **ผลลัพธ์ดีขึ้น"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------SVM1 with feature2-----------------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.77      0.66      0.71      1000\n",
      "        1.0       0.70      0.80      0.75      1000\n",
      "\n",
      "avg / total       0.73      0.73      0.73      2000\n",
      "\n",
      "accuracy = 0.7285\n"
     ]
    }
   ],
   "source": [
    "Tfid2 = TfidfVectorizer(max_df = 0.85 ,min_df = 0.1 )\n",
    "feature2 = Tfid2.fit_transform(raw_data)\n",
    "yhat_SVM2 = cross_val_predict(SVM1,feature2,y)\n",
    "scoreSVM2 = accuracy_score(y,yhat_SVM2)\n",
    "report = classification_report(y, yhat_SVM2)\n",
    "print('--------------------------------SVM1 with feature2-----------------------------------\\n')\n",
    "print(report)\n",
    "print('accuracy = ' + str(scoreSVM2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------SVM1 with feature2-----------------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.80      0.70      0.74      1000\n",
      "        1.0       0.73      0.82      0.77      1000\n",
      "\n",
      "avg / total       0.76      0.76      0.76      2000\n",
      "\n",
      "accuracy = 0.7585\n"
     ]
    }
   ],
   "source": [
    "SVM2 = SVC(kernel = 'sigmoid')\n",
    "yhat_SVM2 = cross_val_predict(SVM2,feature1,y)\n",
    "scoreSVM2 = accuracy_score(y,yhat_SVM2)\n",
    "report = classification_report(y, yhat_SVM2)\n",
    "print('--------------------------------SVM2 with feature1-----------------------------------\\n')\n",
    "print(report)\n",
    "print('accuracy = ' + str(scoreSVM2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------SVM1 with feature2-----------------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.85      0.84      0.84      1000\n",
      "        1.0       0.84      0.85      0.85      1000\n",
      "\n",
      "avg / total       0.85      0.84      0.84      2000\n",
      "\n",
      "accuracy = 0.845\n"
     ]
    }
   ],
   "source": [
    "SVM2 = SVC(kernel = 'linear')\n",
    "yhat_SVM2 = cross_val_predict(SVM2,feature1,y)\n",
    "scoreSVM2 = accuracy_score(y,yhat_SVM2)\n",
    "report = classification_report(y, yhat_SVM2)\n",
    "print('--------------------------------SVM2 with feature1-----------------------------------\\n')\n",
    "print(report)\n",
    "print('accuracy = ' + str(scoreSVM2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP\n",
    "กำหนด max_df = 0.85 min_df = 0.1 \n",
    "    1. accuracy: 0.7655 < 0.8275\n",
    "    2. precission: 0.77 < 0.83\n",
    "    3. recall: 0.77 < 0.83\n",
    "    **max_df และ min_df ทำให้ performance ลดลง \n",
    "เปลี่ยน hyper parameter ค่า hidden_layer_sizes = (200,) จาก Default = (100,) โดย max_df,min_df (default)\n",
    "    1. accuracy: 0.828\n",
    "    2. precission: 0.83\n",
    "    3. recall: 0.83\n",
    "    **accuracy เพิ่มขึ้น อย่างอื่นเท่าเดิม\n",
    "เปลี่ยน hyper parameter ค่า hidden_layer_sizes = (1000,) จาก Default = (100,) โดย max_df,min_df (default)\n",
    "    1. accuracy: 0.829\n",
    "    2. precission: 0.83\n",
    "    3. recall: 0.83\n",
    "    **accuracy เพิ่มขึ้น อย่างอื่นเท่าเดิม\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------MLP1 with feature2-----------------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.77      0.76      0.76      1000\n",
      "        1.0       0.76      0.77      0.77      1000\n",
      "\n",
      "avg / total       0.77      0.77      0.77      2000\n",
      "\n",
      "accuracy = 0.7655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "yhat_MLP2 = cross_val_predict(MLP1,feature2,y)\n",
    "scoreMLP2 = accuracy_score(y,yhat_MLP2)\n",
    "report = classification_report(y, yhat_MLP2)\n",
    "print('--------------------------------MLP1 with feature2-----------------------------------\\n')\n",
    "print(report)\n",
    "print('accuracy = ' + str(scoreMLP2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------MLP1 with feature2-----------------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.84      0.81      0.83      1000\n",
      "        1.0       0.82      0.84      0.83      1000\n",
      "\n",
      "avg / total       0.83      0.83      0.83      2000\n",
      "\n",
      "accuracy = 0.828\n"
     ]
    }
   ],
   "source": [
    "MLP2 = MLPClassifier(hidden_layer_sizes=  (200,))\n",
    "yhat_MLP2 = cross_val_predict(MLP2,feature1,y)\n",
    "scoreMLP2 = accuracy_score(y,yhat_MLP2)\n",
    "report = classification_report(y, yhat_MLP2)\n",
    "print('--------------------------------MLP2 with feature1-----------------------------------\\n')\n",
    "print(report)\n",
    "print('accuracy = ' + str(scoreMLP2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------MLP2 with feature1-----------------------------------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.84      0.81      0.83      1000\n",
      "        1.0       0.82      0.84      0.83      1000\n",
      "\n",
      "avg / total       0.83      0.83      0.83      2000\n",
      "\n",
      "accuracy = 0.829\n"
     ]
    }
   ],
   "source": [
    "MLP2 = MLPClassifier(hidden_layer_sizes=  (1000,))\n",
    "yhat_MLP2 = cross_val_predict(MLP2,feature1,y)\n",
    "scoreMLP2 = accuracy_score(y,yhat_MLP2)\n",
    "report = classification_report(y, yhat_MLP2)\n",
    "print('--------------------------------MLP2 with feature1-----------------------------------\\n')\n",
    "print(report)\n",
    "print('accuracy = ' + str(scoreMLP2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Text Clustering\n",
    "We have heard about Google News clustering. In this exercise, we are going to implement it with Python.\n",
    "\n",
    "## 3.1 Data Preprocessing\n",
    "Let’s switch up and use another dataset called 20newsgroup data, which is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. The data is collected from a university’s mailing list, where students exchange opinions in everything from motorcycles to middle east politics.\n",
    "\n",
    "1. Import data using sklearn.datasets.fetch_20newsgroups \n",
    "2. Transform data to vector with TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data = fetch_20newsgroups(subset = 'train') #select data in subset 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data\n",
    "Tfidf_google = TfidfVectorizer(max_df = 0.8 , min_df = 0.05)\n",
    "x = Tfidf_google.fit_transform(raw_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Clustering\n",
    "We are going to use the simplest clustering model, k-means clustering, to do this task. Our hope is that this simple algorithm will result in meaningful news categories, without using labels.\n",
    "\n",
    "1. Fit K-Means clustering model to the text vector. What is the value of K you should pick? Why?\n",
    "2. Use Silhouette score to evaluate your clusters. Try to evaluate the model for different values of k to see which k fits best for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans\n",
    "    select k = 20 เพราะ data ที่ import เข้ามามี 20 groups\n",
    "Result\n",
    "    1. K = 20 Silhoutte score =  0.00307\n",
    "    2. K = 15 Silhoutte score =  0.00484\n",
    "    3. K = 10 Silhoutte score = -0.00039\n",
    "    4. K = 5  Silhoutte score =  0.00411\n",
    "    5. K = 35 Silhoutte score =  0.00525\n",
    "    6. K = 50 Silhoutte score =  0.00913\n",
    "จากผลลัพธ์ K = 50 ดีที่สุด แต่ผมคิดว่ามันไม่ควรจะแบ่งได้ 50 cluster จึงเลือก K = 15 เพราะใน data จริงๆ จาก 20 group มันสามารถจัดกลุ่มคาวมคล้ายกันทำให้ group น้อยลงได้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------K = 20---------------------------------\n",
      "silhouette score = 0.003071184841869602\n"
     ]
    }
   ],
   "source": [
    "kmean = KMeans(n_clusters = 20) \n",
    "kmean.fit(x)\n",
    "sil_score = silhouette_score(x,kmean.labels_ )\n",
    "print('-----------------------------K = 20---------------------------------')\n",
    "print('silhouette score = '+ str(sil_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------K = 15---------------------------------\n",
      "silhouette score = 0.004844898597086147\n"
     ]
    }
   ],
   "source": [
    "kmean1 = KMeans(n_clusters = 15) \n",
    "kmean1.fit(x)\n",
    "sil_score = silhouette_score(x,kmean1.labels_ )\n",
    "print('-----------------------------K = 15---------------------------------')\n",
    "print('silhouette score = '+ str(sil_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------K = 10---------------------------------\n",
      "silhouette score = -0.0003900338875452058\n"
     ]
    }
   ],
   "source": [
    "kmean2 = KMeans(n_clusters = 10) \n",
    "kmean2.fit(x)\n",
    "sil_score = silhouette_score(x,kmean2.labels_ )\n",
    "print('-----------------------------K = 10---------------------------------')\n",
    "print('silhouette score = '+ str(sil_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------K = 5---------------------------------\n",
      "silhouette score = 0.004115487009679443\n"
     ]
    }
   ],
   "source": [
    "kmean3 = KMeans(n_clusters = 5) \n",
    "kmean3.fit(x)\n",
    "sil_score = silhouette_score(x,kmean3.labels_ )\n",
    "print('-----------------------------K = 5---------------------------------')\n",
    "print('silhouette score = '+ str(sil_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------K = 35---------------------------------\n",
      "silhouette score = 0.005255762323989831\n"
     ]
    }
   ],
   "source": [
    "kmean4 = KMeans(n_clusters = 35) \n",
    "kmean4.fit(x)\n",
    "sil_score = silhouette_score(x,kmean4.labels_ )\n",
    "print('-----------------------------K = 35---------------------------------')\n",
    "print('silhouette score = '+ str(sil_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------K = 50---------------------------------\n",
      "silhouette score = 0.009132529670051202\n"
     ]
    }
   ],
   "source": [
    "kmean5 = KMeans(n_clusters = 50) \n",
    "kmean5.fit(x)\n",
    "sil_score = silhouette_score(x,kmean5.labels_ )\n",
    "print('-----------------------------K = 50---------------------------------')\n",
    "print('silhouette score = '+ str(sil_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Topic Terms\n",
    "We want to explore each cluster to understand what news articles are in the cluster, what terms are associated with the cluster. This will require a bit of hacking.\n",
    "1. Use TfidfVectorizer.get feature names to extract words associated with each dimension of the text vector.\n",
    "2. Extract cluster’s centroids using kmeans.cluster centers .\n",
    "3. For each centroid, print the top 15 words that have the highest frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans clustering K = 15\n",
    "    TOP 15 word in each clusters\n",
    "1. Cluster 0:\n",
    " her\n",
    " she\n",
    " movie\n",
    " you\n",
    " was\n",
    " they\n",
    " when\n",
    " their\n",
    " like\n",
    " about\n",
    " there\n",
    " up\n",
    " so\n",
    " out\n",
    " which\n",
    "2. Cluster 1:\n",
    " movie\n",
    " was\n",
    " you\n",
    " they\n",
    " what\n",
    " just\n",
    " there\n",
    " horror\n",
    " me\n",
    " my\n",
    " like\n",
    " so\n",
    " about\n",
    " or\n",
    " good\n",
    "3. Cluster 2:\n",
    " house\n",
    " tom\n",
    " horror\n",
    " movie\n",
    " you\n",
    " they\n",
    " was\n",
    " what\n",
    " out\n",
    " good\n",
    " just\n",
    " there\n",
    " her\n",
    " up\n",
    " some\n",
    "4. Cluster 3:\n",
    " him\n",
    " life\n",
    " was\n",
    " story\n",
    " more\n",
    " movie\n",
    " which\n",
    " its\n",
    " her\n",
    " we\n",
    " their\n",
    " there\n",
    " when\n",
    " man\n",
    " into\n",
    "5. Cluster 4:\n",
    " jack\n",
    " her\n",
    " ship\n",
    " movie\n",
    " they\n",
    " she\n",
    " him\n",
    " good\n",
    " there\n",
    " you\n",
    " james\n",
    " up\n",
    " more\n",
    " than\n",
    " love\n",
    "6. Cluster 5:\n",
    " you\n",
    " movie\n",
    " your\n",
    " if\n",
    " out\n",
    " about\n",
    " or\n",
    " so\n",
    " her\n",
    " just\n",
    " what\n",
    " there\n",
    " they\n",
    " was\n",
    " when\n",
    "7. Cluster 6:\n",
    " alien\n",
    " ship\n",
    " they\n",
    " movie\n",
    " was\n",
    " which\n",
    " her\n",
    " earth\n",
    " more\n",
    " so\n",
    " its\n",
    " than\n",
    " dr\n",
    " there\n",
    " when\n",
    "8. Cluster 7:\n",
    " they\n",
    " movie\n",
    " there\n",
    " their\n",
    " up\n",
    " joe\n",
    " you\n",
    " out\n",
    " which\n",
    " like\n",
    " comedy\n",
    " funny\n",
    " when\n",
    " can\n",
    " was\n",
    "9. Cluster 8:\n",
    " mr\n",
    " robin\n",
    " which\n",
    " was\n",
    " her\n",
    " no\n",
    " characters\n",
    " there\n",
    " would\n",
    " out\n",
    " while\n",
    " more\n",
    " films\n",
    " character\n",
    " series\n",
    "10. Cluster 9:\n",
    " action\n",
    " movie\n",
    " van\n",
    " you\n",
    " was\n",
    " some\n",
    " like\n",
    " plot\n",
    " so\n",
    " there\n",
    " they\n",
    " good\n",
    " out\n",
    " bad\n",
    " scenes\n",
    "11. Cluster 10:\n",
    " war\n",
    " private\n",
    " men\n",
    " they\n",
    " movie\n",
    " their\n",
    " battle\n",
    " was\n",
    " british\n",
    " american\n",
    " world\n",
    " most\n",
    " mission\n",
    " action\n",
    " about\n",
    "12. Cluster 11:\n",
    " killer\n",
    " you\n",
    " horror\n",
    " murder\n",
    " movie\n",
    " more\n",
    " her\n",
    " was\n",
    " its\n",
    " about\n",
    " what\n",
    " up\n",
    " they\n",
    " too\n",
    " which\n",
    "13. Cluster 12:\n",
    " star\n",
    " effects\n",
    " planet\n",
    " science\n",
    " special\n",
    " fiction\n",
    " movie\n",
    " was\n",
    " space\n",
    " they\n",
    " series\n",
    " so\n",
    " like\n",
    " some\n",
    " we\n",
    "14. Cluster 13:\n",
    " they\n",
    " you\n",
    " we\n",
    " so\n",
    " was\n",
    " about\n",
    " like\n",
    " their\n",
    " there\n",
    " movie\n",
    " what\n",
    " or\n",
    " just\n",
    " up\n",
    " out\n",
    "15. Cluster 14:\n",
    " 10\n",
    " you\n",
    " movie\n",
    " was\n",
    " me\n",
    " also\n",
    " about\n",
    " little\n",
    " some\n",
    " plot\n",
    " my\n",
    " well\n",
    " did\n",
    " see\n",
    " out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat = Tfidf_google.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_centers = kmean1.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " her\n",
      " she\n",
      " movie\n",
      " you\n",
      " was\n",
      " they\n",
      " when\n",
      " their\n",
      " like\n",
      " about\n",
      " there\n",
      " up\n",
      " so\n",
      " out\n",
      " which\n",
      "Cluster 1:\n",
      " movie\n",
      " was\n",
      " you\n",
      " they\n",
      " what\n",
      " just\n",
      " there\n",
      " horror\n",
      " me\n",
      " my\n",
      " like\n",
      " so\n",
      " about\n",
      " or\n",
      " good\n",
      "Cluster 2:\n",
      " house\n",
      " tom\n",
      " horror\n",
      " movie\n",
      " you\n",
      " they\n",
      " was\n",
      " what\n",
      " out\n",
      " good\n",
      " just\n",
      " there\n",
      " her\n",
      " up\n",
      " some\n",
      "Cluster 3:\n",
      " him\n",
      " life\n",
      " was\n",
      " story\n",
      " more\n",
      " movie\n",
      " which\n",
      " its\n",
      " her\n",
      " we\n",
      " their\n",
      " there\n",
      " when\n",
      " man\n",
      " into\n",
      "Cluster 4:\n",
      " jack\n",
      " her\n",
      " ship\n",
      " movie\n",
      " they\n",
      " she\n",
      " him\n",
      " good\n",
      " there\n",
      " you\n",
      " james\n",
      " up\n",
      " more\n",
      " than\n",
      " love\n",
      "Cluster 5:\n",
      " you\n",
      " movie\n",
      " your\n",
      " if\n",
      " out\n",
      " about\n",
      " or\n",
      " so\n",
      " her\n",
      " just\n",
      " what\n",
      " there\n",
      " they\n",
      " was\n",
      " when\n",
      "Cluster 6:\n",
      " alien\n",
      " ship\n",
      " they\n",
      " movie\n",
      " was\n",
      " which\n",
      " her\n",
      " earth\n",
      " more\n",
      " so\n",
      " its\n",
      " than\n",
      " dr\n",
      " there\n",
      " when\n",
      "Cluster 7:\n",
      " they\n",
      " movie\n",
      " there\n",
      " their\n",
      " up\n",
      " joe\n",
      " you\n",
      " out\n",
      " which\n",
      " like\n",
      " comedy\n",
      " funny\n",
      " when\n",
      " can\n",
      " was\n",
      "Cluster 8:\n",
      " mr\n",
      " robin\n",
      " which\n",
      " was\n",
      " her\n",
      " no\n",
      " characters\n",
      " there\n",
      " would\n",
      " out\n",
      " while\n",
      " more\n",
      " films\n",
      " character\n",
      " series\n",
      "Cluster 9:\n",
      " action\n",
      " movie\n",
      " van\n",
      " you\n",
      " was\n",
      " some\n",
      " like\n",
      " plot\n",
      " so\n",
      " there\n",
      " they\n",
      " good\n",
      " out\n",
      " bad\n",
      " scenes\n",
      "Cluster 10:\n",
      " war\n",
      " private\n",
      " men\n",
      " they\n",
      " movie\n",
      " their\n",
      " battle\n",
      " was\n",
      " british\n",
      " american\n",
      " world\n",
      " most\n",
      " mission\n",
      " action\n",
      " about\n",
      "Cluster 11:\n",
      " killer\n",
      " you\n",
      " horror\n",
      " murder\n",
      " movie\n",
      " more\n",
      " her\n",
      " was\n",
      " its\n",
      " about\n",
      " what\n",
      " up\n",
      " they\n",
      " too\n",
      " which\n",
      "Cluster 12:\n",
      " star\n",
      " effects\n",
      " planet\n",
      " science\n",
      " special\n",
      " fiction\n",
      " movie\n",
      " was\n",
      " space\n",
      " they\n",
      " series\n",
      " so\n",
      " like\n",
      " some\n",
      " we\n",
      "Cluster 13:\n",
      " they\n",
      " you\n",
      " we\n",
      " so\n",
      " was\n",
      " about\n",
      " like\n",
      " their\n",
      " there\n",
      " movie\n",
      " what\n",
      " or\n",
      " just\n",
      " up\n",
      " out\n",
      "Cluster 14:\n",
      " 10\n",
      " you\n",
      " movie\n",
      " was\n",
      " me\n",
      " also\n",
      " about\n",
      " little\n",
      " some\n",
      " plot\n",
      " my\n",
      " well\n",
      " did\n",
      " see\n",
      " out\n"
     ]
    }
   ],
   "source": [
    "order_centroids = cluster_centers.argsort()[:, ::-1]\n",
    "for i in range(len(cluster_centers)):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :15]:\n",
    "        print(' %s' % feat[ind])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
